{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Skript for Live Vehicle Data Cleaning\n","\n","<strong><em>Important: This is a guide, which helps and explains you the data cleaning we where doing before this Hack-a-thon. There are parts you can and sometimes should directly copy and paste. You won't be able to copy the whole notebook and run it within your project.</em></strong>\n","\n","## Creating the Client Connection to the Cloud Object Storage and the \"smart-city-live-vehicle-positions\" bucket\n","\n","The following code cell can be automatically inserted trough the Notebook UI. To do so, click on the data button (top right corner) there you find the *files* and *connections* tab. Go to the *connection* as we want to create a client to our Cloud Object Storage. \n","\n","There you will find the Connection which we created before. Click \"insert to code\" and choose the \"StreamingBody object\" option. After that there will open a pop up which showes you the folder structure of your underlying cloud bucket. Choose the right folders and subfolders until you end up in the last subfolder, that contains all the .json files we need. Choose one file and click *Select*. Next you will see a code cell, inserted automatically, that looks like this one except it contains the correct api-keys etc.\n","\n","> It doesn't matter which .json you will choose, because we will later on only use the created client object to access more then only one .json file."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# @hidden_cell\n","\n","\n","# This connection object is used to access your data and contains your credentials or project token.\n","# You might want to remove those credentials before you share your notebook.\n","\n","\n","import types\n","import pandas as pd\n","import ibm_boto3\n","from botocore.client import Config\n","\n","def __iter__(self): return 0\n","\n","# @hidden_cell\n","# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n","# You might want to remove those credentials before you share your notebook.\n","\n","Cloud_Object_Storage_Connection_client = ibm_boto3.client(\n","    service_name='s3',\n","    ibm_api_key_id='api-key',\n","    ibm_service_instance_id='service-instance-id',\n","    ibm_auth_endpoint='https://iam.cloud.ibm.com/identity/token',\n","    config=Config(signature_version='oauth'),\n","    endpoint_url='https://s3.eu-de.cloud-object-storage.appdomain.cloud'\n",")\n","\n","body = Cloud_Object_Storage_Connection_client.get_object(Bucket='smart-city-live-vehicle-positions', Key='topics/open_HFP_API/partition=0/open_HFP_API+0+0054722848.json')['Body']\n","# add missing __iter__ method, so pandas accepts body as file-like object \n","\n","if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n","\n","# Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face an error during data loading.\n","# Refer to the documentation of 'pandas.read_json()' and 'pandas.io.json.json_normalize' for more possibilities to adjust the data loading.\n","# pandas documentation: http://pandas.pydata.org/pandas-docs/stable/io.html#io-json-reader\n","# and http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html\n"]},{"cell_type":"markdown","metadata":{},"source":["After we sucsessfully created the client (your's might be named differently than `Cloud_Object_Storage_Connection_client`, either change it in the cell above or keep in mind to change the name whenever the client is used) we now need a function that can read/get/access more than one .json file. \n","\n","As we know, the data is saved in a S3 object style. The following cell shows the function we use to access files over a given timespan in which they were written to the storage. (Don't stress about the function and how it works in detail ðŸ˜‰) Just copy and paste it.\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import argparse\n","import boto3\n","import dateutil.parser\n","import logging\n","import pytz\n","from collections import namedtuple\n","\n","import pandas as pd\n","from datetime import datetime, timezone, timedelta\n","\n","\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","Rule = namedtuple('Rule', ['has_min', 'has_max'])\n","last_modified_rules = {\n","    Rule(has_min=True, has_max=True):\n","        lambda min_date, date, max_date: min_date <= date <= max_date,\n","    Rule(has_min=True, has_max=False):\n","        lambda min_date, date, max_date: min_date <= date,\n","    Rule(has_min=False, has_max=True):\n","        lambda min_date, date, max_date: date <= max_date,\n","    Rule(has_min=False, has_max=False):\n","        lambda min_date, date, max_date: True,\n","}\n","\n","def get_s3_objects(s3, bucket, prefixes=None, suffixes=None, last_modified_min=None, last_modified_max=None):\n","    \n","    if last_modified_min and last_modified_max and last_modified_max < last_modified_min:\n","        raise ValueError(\n","            \"When using both, last_modified_max: {} must be greater than last_modified_min: {}\".format(\n","                last_modified_max, last_modified_min\n","            )\n","        )\n","    # Use the last_modified_rules dict to lookup which conditional logic to apply\n","    # based on which arguments were supplied\n","    last_modified_rule = last_modified_rules[bool(last_modified_min), bool(last_modified_max)]\n","\n","    if not prefixes:\n","        prefixes = ('',)\n","    else:\n","        prefixes = tuple(set(prefixes))\n","    if not suffixes:\n","        suffixes = ('',)\n","    else:\n","        suffixes = tuple(set(suffixes))\n","\n","    kwargs = {'Bucket': bucket}\n","\n","    for prefix in prefixes:\n","        kwargs['Prefix'] = prefix\n","        while True:\n","            # The S3 API response is a large blob of metadata.\n","            # 'Contents' contains information about the listed objects.\n","            resp = s3.list_objects_v2(**kwargs)\n","            for content in resp.get('Contents', []):\n","                last_modified_date = content['LastModified']\n","                if (\n","                    content['Key'].endswith(suffixes) and\n","                    last_modified_rule(last_modified_min, last_modified_date, last_modified_max)\n","                ):\n","                    yield content\n","\n","            # The S3 API is paginated, returning up to 1000 keys at a time.\n","            # Pass the continuation token into the next response, until we\n","            # reach the final page (when this field is missing).\n","            try:\n","                kwargs['ContinuationToken'] = resp['NextContinuationToken']\n","            except KeyError:\n","                break"]},{"cell_type":"markdown","metadata":{},"source":["## Defining the Timespan, which jsons/S3 objects will be collected\n","\n","As introduced right before this, we are able to access data from LVD trough our client and define a timespan in which we want to get the data.\n","\n","We decided to create three variables:\n","`dateloading`,\n","`starttime`,\n","`endtime`, to create the timespan we were talking about. \n","\n","> Even for a few hours the data that has been collected can sum up to 1.000.000+ rows. So to get a feeling of the data cleaning process it is more than enough to create a small timespan (one hour). Also do remember that it is possible, if you change date and time, that there is no data available."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["dateloading = \"2022-02-26\"\n","starttime = datetime.fromisoformat(dateloading + ' 12:00:00.000+00:00')\n","endtime = datetime.fromisoformat(dateloading + ' 13:00.000+00:00')"]},{"cell_type":"markdown","metadata":{},"source":["Because the data amount is so large, we needed to access the data day by day and within that day in a few hour steps."]},{"cell_type":"markdown","metadata":{},"source":["Trough the `Cloud_Object_Storage_Connection` and with the usage of the defined method `get_s3_objects` we are now able to access our s3 objects, that were written within the defined timewindow."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["objs=get_s3_objects(s3=Cloud_Object_Storage_Connection_client,bucket=\"smart-city-live-vehicle-positions\", last_modified_min=starttime, last_modified_max=endtime)"]},{"cell_type":"markdown","metadata":{},"source":["The variable `objs` is now filled with these s3 Objects, which isn't a format we can really work with in terms of the final data in form of .json. So it requires one more step to extract the wanted data into our pandas.DataFrame."]},{"cell_type":"markdown","metadata":{},"source":["## Reading the Vehicle Positions from the variable objs and store them into a DataFrame"]},{"cell_type":"markdown","metadata":{},"source":["To receive our data, we use the variable `objs` and iterate trough every `obj` that it contains. We have defined an empty DataFrame (`df_lvd`) which will be filled step by step with the data we want to extract. To do so, we have to use our client again. We use `.get_object()`, give it the exact Bucket we want to access and the Key to our data, which is stored in each `obj['Key']['Body']` and read the lines we \"find\" there which are in the .json format. Around that call, we use `pd.read_json()` to extract the data from the json. This input (now in the form of a DataFrame) is now passed to the self defined method `extract_data()` that extracts all the \"VP\" (VehiclePosition) related data, deletes all empty rows, makes a list out of it (to eliminate format issues) and writes it back into a pd.DataFrame form.\n","\n","This is passed back to the iterational loop, where it is appended to the `df_lvd`. After extracting all the jsons from all the given `obj` out of `objs`, we finally reset the index and drop the old one."]},{"cell_type":"code","execution_count":7,"metadata":{"scrolled":true},"outputs":[],"source":["def extract_data(df_temp):\n","    df_temp = df_temp['VP']\n","    df_temp = df_temp.dropna()\n","    df_temp = df_temp.tolist()\n","    df_temp = pd.DataFrame.from_records(df_temp)\n","    return df_temp\n","\n","df_lvd = pd.DataFrame()\n","for obj in objs:\n","    df_lvd = df_lvd.append(extract_data(pd.read_json(Cloud_Object_Storage_Connection_client.get_object(Bucket='smart-city-live-vehicle-positions', Key=obj['Key'])['Body'].read(), lines=True)))   \n","\n","df_lvd = df_lvd.reset_index(drop = True)"]},{"cell_type":"markdown","metadata":{},"source":["Now, let's take a look if that worked out"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>acc</th>\n","      <th>drst</th>\n","      <th>loc</th>\n","      <th>spd</th>\n","      <th>line</th>\n","      <th>jrn</th>\n","      <th>dl</th>\n","      <th>start</th>\n","      <th>hdg</th>\n","      <th>tsi</th>\n","      <th>...</th>\n","      <th>stop</th>\n","      <th>occu</th>\n","      <th>veh</th>\n","      <th>desi</th>\n","      <th>oper</th>\n","      <th>odo</th>\n","      <th>lat</th>\n","      <th>oday</th>\n","      <th>seq</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>GPS</td>\n","      <td>0.00</td>\n","      <td>588</td>\n","      <td>8.0</td>\n","      <td>299.0</td>\n","      <td>10:04</td>\n","      <td>237.0</td>\n","      <td>1646899191</td>\n","      <td>...</td>\n","      <td>1453132</td>\n","      <td>0</td>\n","      <td>820</td>\n","      <td>801</td>\n","      <td>47</td>\n","      <td>0.0</td>\n","      <td>60.209848</td>\n","      <td>2022-03-10</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>GPS</td>\n","      <td>0.00</td>\n","      <td>120</td>\n","      <td>895.0</td>\n","      <td>119.0</td>\n","      <td>10:01</td>\n","      <td>224.0</td>\n","      <td>1646899191</td>\n","      <td>...</td>\n","      <td>1431183</td>\n","      <td>0</td>\n","      <td>1051</td>\n","      <td>86</td>\n","      <td>22</td>\n","      <td>94.0</td>\n","      <td>60.194654</td>\n","      <td>2022-03-10</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.1</td>\n","      <td>0.0</td>\n","      <td>GPS</td>\n","      <td>12.14</td>\n","      <td>964</td>\n","      <td>923.0</td>\n","      <td>-129.0</td>\n","      <td>09:14</td>\n","      <td>63.0</td>\n","      <td>1646899191</td>\n","      <td>...</td>\n","      <td>None</td>\n","      <td>0</td>\n","      <td>41</td>\n","      <td>506</td>\n","      <td>30</td>\n","      <td>12966.0</td>\n","      <td>60.233694</td>\n","      <td>2022-03-10</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.5</td>\n","      <td>NaN</td>\n","      <td>GPS</td>\n","      <td>12.08</td>\n","      <td>769</td>\n","      <td>57.0</td>\n","      <td>-166.0</td>\n","      <td>09:40</td>\n","      <td>335.0</td>\n","      <td>1646899191</td>\n","      <td>...</td>\n","      <td>None</td>\n","      <td>0</td>\n","      <td>1124</td>\n","      <td>322</td>\n","      <td>22</td>\n","      <td>NaN</td>\n","      <td>60.210779</td>\n","      <td>2022-03-10</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.1</td>\n","      <td>NaN</td>\n","      <td>GPS</td>\n","      <td>38.12</td>\n","      <td>284</td>\n","      <td>9651.0</td>\n","      <td>-50.0</td>\n","      <td>09:40</td>\n","      <td>25.0</td>\n","      <td>1646899191</td>\n","      <td>...</td>\n","      <td>None</td>\n","      <td>0</td>\n","      <td>6317</td>\n","      <td>R</td>\n","      <td>90</td>\n","      <td>NaN</td>\n","      <td>60.367697</td>\n","      <td>2022-03-10</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>111027</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>GPS</td>\n","      <td>0.00</td>\n","      <td>244</td>\n","      <td>452.0</td>\n","      <td>-346.0</td>\n","      <td>09:12</td>\n","      <td>234.0</td>\n","      <td>1646899313</td>\n","      <td>...</td>\n","      <td>1130113.0</td>\n","      <td>0</td>\n","      <td>1015</td>\n","      <td>212</td>\n","      <td>22</td>\n","      <td>19654.0</td>\n","      <td>60.170823</td>\n","      <td>2022-03-10</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>111028</th>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>ODO</td>\n","      <td>NaN</td>\n","      <td>636</td>\n","      <td>8636.0</td>\n","      <td>59.0</td>\n","      <td>09:28</td>\n","      <td>NaN</td>\n","      <td>1646899313</td>\n","      <td>...</td>\n","      <td>4530501.0</td>\n","      <td>0</td>\n","      <td>1035</td>\n","      <td>P</td>\n","      <td>90</td>\n","      <td>25909.0</td>\n","      <td>NaN</td>\n","      <td>2022-03-10</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>111029</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>GPS</td>\n","      <td>0.00</td>\n","      <td>973</td>\n","      <td>921.0</td>\n","      <td>299.0</td>\n","      <td>09:35</td>\n","      <td>94.0</td>\n","      <td>1646899309</td>\n","      <td>...</td>\n","      <td>1370108.0</td>\n","      <td>0</td>\n","      <td>833</td>\n","      <td>61T</td>\n","      <td>6</td>\n","      <td>10204.0</td>\n","      <td>60.241650</td>\n","      <td>2022-03-10</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>111030</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>GPS</td>\n","      <td>0.00</td>\n","      <td>973</td>\n","      <td>921.0</td>\n","      <td>299.0</td>\n","      <td>09:35</td>\n","      <td>94.0</td>\n","      <td>1646899310</td>\n","      <td>...</td>\n","      <td>1370108.0</td>\n","      <td>0</td>\n","      <td>833</td>\n","      <td>61T</td>\n","      <td>6</td>\n","      <td>10204.0</td>\n","      <td>60.241650</td>\n","      <td>2022-03-10</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>111031</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>GPS</td>\n","      <td>0.00</td>\n","      <td>973</td>\n","      <td>921.0</td>\n","      <td>299.0</td>\n","      <td>09:35</td>\n","      <td>94.0</td>\n","      <td>1646899311</td>\n","      <td>...</td>\n","      <td>1370108.0</td>\n","      <td>0</td>\n","      <td>833</td>\n","      <td>61T</td>\n","      <td>6</td>\n","      <td>10204.0</td>\n","      <td>60.241650</td>\n","      <td>2022-03-10</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>111032 rows Ã— 24 columns</p>\n","</div>"],"text/plain":["        acc  drst  loc    spd line     jrn     dl  start    hdg         tsi  \\\n","0       0.0   0.0  GPS   0.00  588     8.0  299.0  10:04  237.0  1646899191   \n","1       0.0   1.0  GPS   0.00  120   895.0  119.0  10:01  224.0  1646899191   \n","2       0.1   0.0  GPS  12.14  964   923.0 -129.0  09:14   63.0  1646899191   \n","3       0.5   NaN  GPS  12.08  769    57.0 -166.0  09:40  335.0  1646899191   \n","4      -0.1   NaN  GPS  38.12  284  9651.0  -50.0  09:40   25.0  1646899191   \n","...     ...   ...  ...    ...  ...     ...    ...    ...    ...         ...   \n","111027  0.0   1.0  GPS   0.00  244   452.0 -346.0  09:12  234.0  1646899313   \n","111028  NaN   1.0  ODO    NaN  636  8636.0   59.0  09:28    NaN  1646899313   \n","111029  0.0   1.0  GPS   0.00  973   921.0  299.0  09:35   94.0  1646899309   \n","111030  0.0   1.0  GPS   0.00  973   921.0  299.0  09:35   94.0  1646899310   \n","111031  0.0   1.0  GPS   0.00  973   921.0  299.0  09:35   94.0  1646899311   \n","\n","        ...       stop  occu   veh desi oper      odo        lat        oday  \\\n","0       ...    1453132     0   820  801   47      0.0  60.209848  2022-03-10   \n","1       ...    1431183     0  1051   86   22     94.0  60.194654  2022-03-10   \n","2       ...       None     0    41  506   30  12966.0  60.233694  2022-03-10   \n","3       ...       None     0  1124  322   22      NaN  60.210779  2022-03-10   \n","4       ...       None     0  6317    R   90      NaN  60.367697  2022-03-10   \n","...     ...        ...   ...   ...  ...  ...      ...        ...         ...   \n","111027  ...  1130113.0     0  1015  212   22  19654.0  60.170823  2022-03-10   \n","111028  ...  4530501.0     0  1035    P   90  25909.0        NaN  2022-03-10   \n","111029  ...  1370108.0     0   833  61T    6  10204.0  60.241650  2022-03-10   \n","111030  ...  1370108.0     0   833  61T    6  10204.0  60.241650  2022-03-10   \n","111031  ...  1370108.0     0   833  61T    6  10204.0  60.241650  2022-03-10   \n","\n","        seq  label  \n","0       NaN    NaN  \n","1       NaN    NaN  \n","2       NaN    NaN  \n","3       NaN    NaN  \n","4       NaN    NaN  \n","...     ...    ...  \n","111027  NaN    NaN  \n","111028  NaN    NaN  \n","111029  NaN    NaN  \n","111030  NaN    NaN  \n","111031  NaN    NaN  \n","\n","[111032 rows x 24 columns]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["df_lvd"]},{"cell_type":"markdown","metadata":{},"source":["How many rows x columns do we have at hand? "]},{"cell_type":"code","execution_count":9,"metadata":{"scrolled":true},"outputs":[{"data":{"text/plain":["(111032, 24)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df_lvd.shape"]},{"cell_type":"markdown","metadata":{},"source":["# Clean the DataFrame\n","\n","Let's start with the cleaning process of the data! :) \n","\n","## Delete rows with missing information about location\n","\n","So LVD data is all about Vehicle Position data. So obviously, every record without the information about the position / geo-location is not interesting for us. We start the cleaning procedure with eliminating all rows that don't contain a `lat` or `long` (latitude, longitude)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["df_lvd = df_lvd[df_lvd['lat'].notna() & df_lvd['long'].notna()]"]},{"cell_type":"markdown","metadata":{},"source":["## Deciding which featuers/columns are relevant\n","\n","Not all of the 24 columns, or so called features, are relevant to us. To determine which are and which might not be, we take a look at the distribution of each feature (how many missing values) and look into the documentary about the API from where the data originally came.\n","(https://digitransit.fi/en/developers/apis/4-realtime-api/vehicle-positions/) \n","\n","Let's start with the calculation about the data distribution:"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["acc has this many NA values: 4.624852762584805%\n","drst has this many NA values: 16.740779970232932%\n","loc has this many NA values: 0.0%\n","spd has this many NA values: 0.06756941844645123%\n","line has this many NA values: 3.760146826520084%\n","jrn has this many NA values: 4.5572833441383525%\n","dl has this many NA values: 9.742779659778847%\n","start has this many NA values: 0.0%\n","hdg has this many NA values: 1.4545686970972542%\n","tsi has this many NA values: 0.0%\n","dir has this many NA values: 0.0%\n","long has this many NA values: 0.0%\n","route has this many NA values: 0.0%\n","tst has this many NA values: 0.0%\n","stop has this many NA values: 58.72056393071395%\n","occu has this many NA values: 0.0%\n","veh has this many NA values: 0.0%\n","desi has this many NA values: 0.0%\n","oper has this many NA values: 0.0%\n","odo has this many NA values: 19.650830464676716%\n","lat has this many NA values: 0.0%\n","oday has this many NA values: 0.0%\n","seq has this many NA values: 95.49567647031968%\n","label has this many NA values: 99.94704018554197%\n"]}],"source":["for column in df_lvd.columns:\n","    percent = (df_lvd[column].isna().sum()/len(df_lvd))*100\n","    print(column + \" has this many NA values: \" + str(percent) + \"%\")"]},{"cell_type":"markdown","metadata":{},"source":["Now seeing, that there are columns which have a high percentage of missing values, we derive action from it.\n","We decide to delete `seq`and `label` because they're each missing in more than 95% of the total rows. \n","\n","After looking at the documentation, we are going to decide that we want to delete `odo` because it's value is not reliable. `oday`, `line` and `jrn` are listed as values, with an internal only purpose, so we are going to delete them too. \n","\n","**Now take a moment, look into the documentation yourself and decide what else you might want to clean from your data set. Maybe you take the analytical question etc. into account and think about the goal you want to achieve with the given data.**"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["   acc  drst  loc    spd     dl  start    hdg         tsi dir       long  \\\n","0  0.0   0.0  GPS   0.00  299.0  10:04  237.0  1646899191   1  25.077927   \n","1  0.0   1.0  GPS   0.00  119.0  10:01  224.0  1646899191   1  25.031280   \n","2  0.1   0.0  GPS  12.14 -129.0  09:14   63.0  1646899191   2  25.030158   \n","3  0.5   NaN  GPS  12.08 -166.0  09:40  335.0  1646899191   1  24.888061   \n","4 -0.1   NaN  GPS  38.12  -50.0  09:40   25.0  1646899191   1  25.090595   \n","\n","   route                       tst     stop  occu   veh desi  oper        lat  \n","0   1801  2022-03-10T07:59:51.305Z  1453132     0   820  801    47  60.209848  \n","1   1086  2022-03-10T07:59:51.309Z  1431183     0  1051   86    22  60.194654  \n","2   1506  2022-03-10T07:59:51.308Z     None     0    41  506    30  60.233694  \n","3   4322  2022-03-10T07:59:51.312Z     None     0  1124  322    22  60.210779  \n","4  3001R  2022-03-10T07:59:51.258Z     None     0  6317    R    90  60.367697  \n"]}],"source":["if 'seq' in df_lvd.columns:\n","    df_lvd_cleaned = df_lvd.drop(['seq', 'label', 'odo', 'oday', 'jrn', 'line'], axis=1)\n","else:\n","    df_lvd_cleaned = df_lvd.drop(['label', 'odo', 'oday', 'jrn', 'line'], axis=1)\n","        \n","\n","df_lvd_cleaned = df_lvd_cleaned.reset_index(drop=True)\n","print(df_lvd_cleaned.head())"]},{"cell_type":"markdown","metadata":{},"source":["## Important infos about the features:\n","- spd => in m/s\n","- acc => m/s^2\n","- dl => delay in sec\n","- drst => door status (0 = closed, 1 = open)\n","- ... Feel free to expand "]},{"cell_type":"markdown","metadata":{},"source":["When we take a closer look at the documentation you will find that there are two ways, the location is defined. Beside `GPS`, there is also `ODO` and `MAN`. We normally don't \"trust\" the estimation of an location nor do we wan't some manual inserted information. But before we consider deleting these, we check how the `GPS` based locating technique is used."]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["95.1240446688642"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["(df_lvd_cleaned['loc'] == \"GPS\").sum() / len(df_lvd_cleaned) * 100"]},{"cell_type":"markdown","metadata":{},"source":["We see that just over 95% are `GPS` based information. That's definatly an overwhelming majority, so we drop the others"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["df_lvd_cleaned = df_lvd_cleaned[df_lvd_cleaned['loc'] == \"GPS\"]\n","df_lvd_cleaned = df_lvd_cleaned.reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{},"source":["Since every location is now measured trough `GPS` this column doesn't provide additional information, so we drop the whole column."]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["df_lvd_cleaned = df_lvd_cleaned.drop(['loc'], axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["Now our dataframe looks like this."]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>acc</th>\n","      <th>drst</th>\n","      <th>spd</th>\n","      <th>dl</th>\n","      <th>start</th>\n","      <th>hdg</th>\n","      <th>tsi</th>\n","      <th>dir</th>\n","      <th>long</th>\n","      <th>route</th>\n","      <th>tst</th>\n","      <th>stop</th>\n","      <th>occu</th>\n","      <th>veh</th>\n","      <th>desi</th>\n","      <th>oper</th>\n","      <th>lat</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>299.0</td>\n","      <td>10:04</td>\n","      <td>237.0</td>\n","      <td>1646899191</td>\n","      <td>1</td>\n","      <td>25.077927</td>\n","      <td>1801</td>\n","      <td>2022-03-10T07:59:51.305Z</td>\n","      <td>1453132</td>\n","      <td>0</td>\n","      <td>820</td>\n","      <td>801</td>\n","      <td>47</td>\n","      <td>60.209848</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.00</td>\n","      <td>119.0</td>\n","      <td>10:01</td>\n","      <td>224.0</td>\n","      <td>1646899191</td>\n","      <td>1</td>\n","      <td>25.031280</td>\n","      <td>1086</td>\n","      <td>2022-03-10T07:59:51.309Z</td>\n","      <td>1431183</td>\n","      <td>0</td>\n","      <td>1051</td>\n","      <td>86</td>\n","      <td>22</td>\n","      <td>60.194654</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.1</td>\n","      <td>0.0</td>\n","      <td>12.14</td>\n","      <td>-129.0</td>\n","      <td>09:14</td>\n","      <td>63.0</td>\n","      <td>1646899191</td>\n","      <td>2</td>\n","      <td>25.030158</td>\n","      <td>1506</td>\n","      <td>2022-03-10T07:59:51.308Z</td>\n","      <td>None</td>\n","      <td>0</td>\n","      <td>41</td>\n","      <td>506</td>\n","      <td>30</td>\n","      <td>60.233694</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.5</td>\n","      <td>NaN</td>\n","      <td>12.08</td>\n","      <td>-166.0</td>\n","      <td>09:40</td>\n","      <td>335.0</td>\n","      <td>1646899191</td>\n","      <td>1</td>\n","      <td>24.888061</td>\n","      <td>4322</td>\n","      <td>2022-03-10T07:59:51.312Z</td>\n","      <td>None</td>\n","      <td>0</td>\n","      <td>1124</td>\n","      <td>322</td>\n","      <td>22</td>\n","      <td>60.210779</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.1</td>\n","      <td>NaN</td>\n","      <td>38.12</td>\n","      <td>-50.0</td>\n","      <td>09:40</td>\n","      <td>25.0</td>\n","      <td>1646899191</td>\n","      <td>1</td>\n","      <td>25.090595</td>\n","      <td>3001R</td>\n","      <td>2022-03-10T07:59:51.258Z</td>\n","      <td>None</td>\n","      <td>0</td>\n","      <td>6317</td>\n","      <td>R</td>\n","      <td>90</td>\n","      <td>60.367697</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   acc  drst    spd     dl  start    hdg         tsi dir       long  route  \\\n","0  0.0   0.0   0.00  299.0  10:04  237.0  1646899191   1  25.077927   1801   \n","1  0.0   1.0   0.00  119.0  10:01  224.0  1646899191   1  25.031280   1086   \n","2  0.1   0.0  12.14 -129.0  09:14   63.0  1646899191   2  25.030158   1506   \n","3  0.5   NaN  12.08 -166.0  09:40  335.0  1646899191   1  24.888061   4322   \n","4 -0.1   NaN  38.12  -50.0  09:40   25.0  1646899191   1  25.090595  3001R   \n","\n","                        tst     stop  occu   veh desi  oper        lat  \n","0  2022-03-10T07:59:51.305Z  1453132     0   820  801    47  60.209848  \n","1  2022-03-10T07:59:51.309Z  1431183     0  1051   86    22  60.194654  \n","2  2022-03-10T07:59:51.308Z     None     0    41  506    30  60.233694  \n","3  2022-03-10T07:59:51.312Z     None     0  1124  322    22  60.210779  \n","4  2022-03-10T07:59:51.258Z     None     0  6317    R    90  60.367697  "]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["df_lvd_cleaned.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Converting the tst into a other timestamp format \n","\n","We want to do this, because\n","1. SPSS Modeller don't understand the given format in `tst`\n","2. We maybe want to enrich the data by JOIN it with another resource.\n","   \n","So we define a method, which takes every `tst` value and save a changed version of that into a list, which becomes a new column after that."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import datetime\n","\n","date = []\n","hour = []\n","\n","for x in df_lvd_cleaned.tst:\n","    x = x[:-11]\n","    date_obj = datetime.datetime.strptime(x, '%Y-%m-%dT%H')\n","    date.append(str(date_obj.date()) + \" \" + str(date_obj.time()))\n","    \n","date"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["df_lvd_cleaned['timestamp'] = date"]},{"cell_type":"markdown","metadata":{},"source":["# Save the data back to our project"]},{"cell_type":"markdown","metadata":{},"source":["If we think we are finished with cleaning our data, which doesn't have to mean you have the exact same result, we want to extract the data out of the notebook and back into our project space. There we can use it as a data asset. \n","\n","To do so, we use the python libary `project_lib` and import `Project` from it. This gives us the needed functionality, to save the data (dataFrame is converted trough a pandas.DataFrame method named `to_csv()` into a csv format) back to our project space where it can be found as an data asset. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# @hidden_cell\n","# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\n","from project_lib import Project\n","project = Project(project_id='id', project_access_token='access-token')\n","pc = project.project_context\n","\n","project.save_data(data=df_lvd_cleaned.to_csv(index=False),file_name=str(dateloading)+'_only_lvd.csv',overwrite=True)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":1}
